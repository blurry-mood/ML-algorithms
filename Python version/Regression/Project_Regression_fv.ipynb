{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project-Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOb/oeaFTMTeepGTpYgX+c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAyyoouubb/TP_ML/blob/master/Project_Regression_fv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhDI7tI7os0-"
      },
      "source": [
        "Importing libraries,\n",
        "* **numpy** for matrix structures\n",
        "* **pandas** for reading tables\n",
        "* **matplotlib**  for visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDbF5XzJl6ut"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.plotting import scatter_matrix as sm\n",
        "import random\n",
        "%matplotlib inline\n",
        "import warnings  \n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJhEmhVPjUd-"
      },
      "source": [
        "### **First Part:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbzIfvh9lnXL"
      },
      "source": [
        "\n",
        "\n",
        "Pandas function `read_csv()` is used to read the csv file **data8.csv** and place it as a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr_49wUHj55X",
        "outputId": "d3896ee5-4ec7-4d91-c428-3cc60373f142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "data =pd.read_csv(\"drive/MyDrive/project_machinglearning_S3/data8.csv\")\n",
        "data= data.sample(frac=1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-163ea8e36f36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/MyDrive/project_machinglearning_S3/data8.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive/project_machinglearning_S3/data8.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zln-wjvrf3B1"
      },
      "source": [
        "Split the dataset into inputs(x) and output(y). Use the method values to transform from a DataFrame object to an array object, which can efficiently managed by Numpy library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zEeEKETf1d3"
      },
      "source": [
        "y=data.y.values.reshape(-1,1)\n",
        "\n",
        "x=data[\"x\"].values.reshape(-1,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3eLXKfKgKaT"
      },
      "source": [
        "Add a bias column to the input vector. This is a column of ones so when we calibrate the parameters it will also multiply such bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0ePGEa9gKoL"
      },
      "source": [
        "X = np.concatenate((np.ones((len(x),1)),x),axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NAHuAY4j6Cn"
      },
      "source": [
        "Spliting the data into **80%** training set and **20%** testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTcAPt9kj6NP"
      },
      "source": [
        "trainingSplit = int(.8 * data.shape[0])\n",
        "\n",
        "x_train = x[:trainingSplit]\n",
        "x_test = x[trainingSplit:]\n",
        "\n",
        "X_train = X[:trainingSplit]\n",
        "X_test = X[trainingSplit:]\n",
        "\n",
        "y_train = y[:trainingSplit] \n",
        "y_test = y[trainingSplit:]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqgs81k8j6dH"
      },
      "source": [
        "There are one feature x and one column y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZtZFC1wj6m8"
      },
      "source": [
        "print(*data.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6ugKzNykLeu"
      },
      "source": [
        "print(x_train.shape,y_train.shape)\n",
        "print(x_test.shape,X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlxVl6S1mw8O"
      },
      "source": [
        "No invalid value was found in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnbBS_U8fUZU"
      },
      "source": [
        "pd.isna(data).any()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31wYJjIFm4-D"
      },
      "source": [
        "The describtion of the data: \n",
        "\n",
        "feature| mean | median | min | max\n",
        "--- | --- | --- | --- | ---\n",
        "x | 11.45|-0.1 |0|22.9\n",
        "y | 25.94| 157.93|-0.43|50.44"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNon3M2CkPQX"
      },
      "source": [
        "print(data.describe())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRINi5IYlOFv"
      },
      "source": [
        "Ploting the training set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHZxEB-JkiyV"
      },
      "source": [
        "plt.scatter(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-xconWnectz"
      },
      "source": [
        "Create a function `grad()` to compute the necessary gradients of the cost function. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARBoONmOse6F"
      },
      "source": [
        "m = data.shape[0]\n",
        "\n",
        "# calculate gradient\n",
        "def grad(theta,X,y):\n",
        "    dJ = 1/m*np.sum((X.dot(theta)-y)*X,axis=0).reshape(-1,1)\n",
        "    return dJ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33VaK64Kg7Sy"
      },
      "source": [
        "Similarly, calculate the cost function `cost()`, also known as objective function, which can be expressed as the sum of the squared errors, as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH8pplhk4WIP"
      },
      "source": [
        "def cost(theta,X,y):\n",
        "    J = np.sum((X.dot(theta)-y)**2,axis=0)[0]/m\n",
        "    return J\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLqP5tGchGQv"
      },
      "source": [
        "Armijo function for linear search the learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOTIb62K_Dha"
      },
      "source": [
        "def armijo(X,y,theta,grad=grad,cost=cost):\n",
        "    eps = .001\n",
        "    eta = 10\n",
        "    alpha =10e-15    \n",
        "    gradient = grad(theta,X,y)\n",
        "    phiprzero = -gradient.T.dot(gradient)\n",
        "    phizero = cost(theta,X,y)\n",
        "    while  cost(theta-alpha*gradient,X,y)<= alpha * phiprzero * eps + phizero :\n",
        "      alpha*=eta\n",
        "    return alpha/eta\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDOFUw9UhQEu"
      },
      "source": [
        "We are ready to implement the Gradient Descent algorithm\n",
        "The parametres :\n",
        "* `X` features;\n",
        "* `y` labels;\n",
        "* `learning_rate` learning rate for parametres update;\n",
        "* `epochs` max iteration of gradient descent update;\n",
        "* `TOL` is a tolerance, i.e a maximum difference between the values of the parameters between iterations so it can be stated that the values converged;\n",
        "* `use_armijo` True to opt the armijo rule to calculate the learning rate otherwise the algorithm will use the fixed learning rate.\n",
        "\n",
        "\n",
        "The steps of this algorithm consists of:\n",
        "* Obtain the gradients of the cost function according the actual value of the parameters;\n",
        "* Calculate the cost to keep track of it;\n",
        "* Update the parameters;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNdYbYW54kAH"
      },
      "source": [
        "def GD(X,y,learning_rate = 0.001,epochs=100,TOL=1e-7,use_armijo = False,verbose=False):\n",
        "    theta = np.array([[0]]*X.shape[1])\n",
        "    if verbose:print(theta)\n",
        "    theta_history = [theta]\n",
        "    J_history = [cost(theta,X,y)]\n",
        "    \n",
        "    thetanew = theta\n",
        "    if verbose:print(f'epoch \\t Cost(J,X,y) \\t')\n",
        "    for epoch in range(epochs):\n",
        "        if epoch%100 == 0:\n",
        "            if verbose:print(f'{epoch:5d}\\t{J_history[-1]:7.4f}\\t')\n",
        "        dJ = grad(theta,X,y)\n",
        "        J = cost(theta,X,y)\n",
        "        if use_armijo:learning_rate= armijo(X,y,theta)\n",
        "        thetanew = theta - learning_rate*dJ\n",
        "        theta_history.append(thetanew)\n",
        "        J_history.append(J)\n",
        "        \n",
        "        if np.sum((thetanew - theta)**2) < TOL:\n",
        "            if verbose:print('Convergence achieved.')\n",
        "            break\n",
        "        theta = thetanew\n",
        "    if verbose:print(theta)\n",
        "    return thetanew,theta_history,J_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2XpMbVCnVmn"
      },
      "source": [
        "Create and train the 4 models ( learning rate = 0.001,0.002,0.003 and using armijo rule)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmhJ2tfHi1AX"
      },
      "source": [
        "models = []\n",
        "models.append( GD(X_train,y_train,learning_rate = 0.001))\n",
        "models.append( GD(X_train,y_train,learning_rate = 0.002))\n",
        "models.append( GD(X_train,y_train,learning_rate = 0.003))\n",
        "models.append( GD(X_train,y_train,use_armijo=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j0Q6ErCjWW-"
      },
      "source": [
        "Drawing the fit line for the 4 models;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YygKQ4J049IE"
      },
      "source": [
        "plt.scatter(x_train, y_train)\n",
        "\n",
        "colors = ['red' , 'blue', \"green\",\"black\"]\n",
        "labels = ['0.001','0.002','0.003','Armijo']\n",
        "for label,color,model in zip(labels,colors,models):\n",
        "  theta = model[0]\n",
        "  Y_pred = X_train.dot(theta)\n",
        "  plt.plot([min(x_train), max(x_train)], [min(Y_pred), max(Y_pred)], color=color,label = label) # predicted\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaSg-WZh0jUS"
      },
      "source": [
        "colors = ['red' , 'blue', \"green\",\"black\"]\n",
        "labels = ['0.001','0.002','0.003','Armijo']\n",
        "for label,color,model in zip(labels,colors,models):\n",
        "  J_history = model[2]\n",
        "  plt.plot(J_history, color=color,label = label) # predicted\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOg9rGb5kNR9"
      },
      "source": [
        "Generalization error  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RACMcbykYqw"
      },
      "source": [
        "for model in models:\n",
        "  theta =model[0]\n",
        "  print(cost(theta,X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHyISvUm94h-"
      },
      "source": [
        "R²  score or the coefficient of determination explains how much the total variance of the dependent variable can be reduced by using the least square regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lTn4CF46rsB"
      },
      "source": [
        "def accuracy(y_actual,y_pred):\n",
        "  # sum of square of residuals\n",
        "  ssr = np.sum((y_pred - y_actual)**2)\n",
        "  #  total sum of squares\n",
        "  sst = np.sum((y_actual - np.mean(y_actual))**2)\n",
        "  # R2 score\n",
        "  r2_score = 1 - (ssr/sst)\n",
        "  return r2_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbGihCnPnpPN"
      },
      "source": [
        "The accuracy of the 4 models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jim_s0r39x8G"
      },
      "source": [
        "for model in models:\n",
        "  theta =model[0]\n",
        "  print(accuracy(y,X.dot(theta)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkrRnjG3N3Xy"
      },
      "source": [
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89zJGPclmD-y"
      },
      "source": [
        "# Bias and variance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YnJ7HcHK65i"
      },
      "source": [
        "def bootstrap(samples, X,y):\n",
        "  X_train_sets = []\n",
        "  y_train_sets = []\n",
        "\n",
        "  ix = int(0.7*X.shape[0])\n",
        "  for sample in range(samples):\n",
        "    ix = np.random.randint(X.shape[0],size=X.shape[0])\n",
        "\n",
        "    xx= X[ix,:] \n",
        "    yy= y[ix,:]\n",
        "\n",
        "    X_train_sets.append(xx)\n",
        "    y_train_sets.append(yy)\n",
        "\n",
        "  return X_train_sets,y_train_sets\n",
        "\n",
        "X_train_sets,y_train_sets = bootstrap(10,X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WThB9QZYa6_J"
      },
      "source": [
        "def bias_var(X_train_sets,y_train_sets,X_test,y_test):\n",
        "  samples = len(X_train_sets)\n",
        "  predicts = np.zeros((samples,y_test.shape[0],1))\n",
        "  for ix in range(samples):\n",
        "    theta,theta_history,J_history = GD(X_train_sets[ix],y_train_sets[ix],use_armijo=1)\n",
        "    predicts[ix,:] =X_test.dot(theta)\n",
        "\n",
        "  mean_preds = np.mean(predicts)\n",
        "  bias_per_point = (mean_preds-y_test)**2\n",
        "  bias = np.mean(bias_per_point)\n",
        "  var_per_point = np.mean((predicts-mean_preds)**2,axis=0)\n",
        "  var = np.mean(var_per_point)\n",
        "  return bias,var\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt4roevH5oNh"
      },
      "source": [
        "bias,var = bias_var(X_train_sets,y_train_sets,X_test,y_test)\n",
        "print(\"bias = \",bias,\" variance =\",var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My2IXdbWj1Mh"
      },
      "source": [
        "### **Second Part**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVw1ShvFdw2B"
      },
      "source": [
        "data = pd.read_csv(\"drive/MyDrive/project_machinglearning_S3/data4.csv\")\n",
        "\n",
        "data= data.sample(frac=1)\n",
        "\n",
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKwx_XGaqpwb"
      },
      "source": [
        "Spliting to traing and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PBsamQdquJV"
      },
      "source": [
        "y=data.y.values.reshape(-1,1)\n",
        "x=data[\"x\"].values.reshape(-1,1)\n",
        "\n",
        "print(x.shape,y.shape)\n",
        "trainingSplit = int(.8 * data.shape[0])\n",
        "\n",
        "\n",
        "x_train = x[:trainingSplit]\n",
        "y_train = y[:trainingSplit] \n",
        "x_test = x[trainingSplit:]\n",
        "y_test = y[trainingSplit:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmsXtFTMp0Kz"
      },
      "source": [
        " The data contain 2 features `x` and `y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKhC9dCPnyOm"
      },
      "source": [
        "print(*data.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNQycAw2rNJ1"
      },
      "source": [
        "The size of training set is **192**, the size of testing size is **48**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQc0OCtUrP2S"
      },
      "source": [
        "print(x_train.shape,y_train.shape)\n",
        "print(x_test.shape,X_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le8TdH12sLUH"
      },
      "source": [
        "The describtion of our dataset :\n",
        "\n",
        " feature| mean | median | min | max\n",
        "--- | --- | --- | --- | ---\n",
        "x | -0.1|-0.1 |-24|23.8\n",
        "y | 193.31| 157.39|-67.13|578.49"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWDmnUvmtGTI"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqB77zBNwhrV"
      },
      "source": [
        "Using `matplotlib.scatter` to plot our **2D** dataset as scatters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1weoEPwxMJg"
      },
      "source": [
        "plt.scatter(x_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85EsZinBFdKY"
      },
      "source": [
        "Create the tranformation `tf` which take the initial features and the degree and give the new features according to pylinomial transformation of given degree.\n",
        "\n",
        "we will use the same gradient descent implimentated in part 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV2inqZvhB3O"
      },
      "source": [
        "def tf(x,deg):\n",
        "  X=np.ones((len(x),1))\n",
        "  for i in range(1,deg+1):\n",
        "    X = np.concatenate((X,x**i),axis=1)\n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ggw64Dvx0Yo"
      },
      "source": [
        "Apply the tranformation for degree = 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvr0i2Z7P2ty"
      },
      "source": [
        "k=2\n",
        "X_train =tf(x_train,deg=k)\n",
        "X_test =tf(x_test,deg=k)\n",
        "X =tf(x,deg=k)\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QtnAAlZHkio"
      },
      "source": [
        "**Q4.** Fiting the model;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TygjbdWbGWS7"
      },
      "source": [
        "\n",
        "theta,theta_history,J_history = GD(X_train,y_train,use_armijo=1)\n",
        "print(\"Error generalization =\",cost(theta,X_test,X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz_P-8FX4Umx"
      },
      "source": [
        "Draw result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Jskafi4TKR"
      },
      "source": [
        "plt.scatter(x_train, y_train)\n",
        "Y_pred = X_train.dot(theta)\n",
        "plt.scatter(x_train, Y_pred, color=\"red\") # predicted\n",
        "plt.title(\"degree = \"+str(k))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbUvCkmFLdX0"
      },
      "source": [
        "y_pred =X_test.dot(theta)\n",
        "print(\"Accurcy = \",accuracy(X_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YDtaJGN0cIY"
      },
      "source": [
        "plt.plot(J_history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wObDkBtaSF2g"
      },
      "source": [
        "Bias varience estimation for the 4 models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHdRoPbUe4IJ"
      },
      "source": [
        "for d in [1,2,4]:\n",
        "  X_train = tf(x_train,deg = d)\n",
        "  X_test = tf(x_test,deg = d)\n",
        "  X_train_sets,y_train_sets = bootstrap(10,X_train,y_train)\n",
        "  bias,var = bias_var(X_train_sets,y_train_sets,X_test,y_test)\n",
        "  print(\"deg \",d,\"bias\",bias,\"var\",var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTFJPlqOI37U"
      },
      "source": [
        "### PART3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpdOUUXMJEVg"
      },
      "source": [
        "To apply the logistic regression we need to normalise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG2XYZSNHrIy"
      },
      "source": [
        "ynorm = y.copy()\n",
        "maxy = np.max(y)\n",
        "miny = np.min(y)\n",
        "ynorm = (y-miny)/(maxy - miny) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr2EgjbNRo16"
      },
      "source": [
        "Create the function for the logistic model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq3sAcq_LYci"
      },
      "source": [
        "def sigmoid(z): return 1 / (1 + np.e**(-z))\n",
        "m = data.shape[0]\n",
        "\n",
        "# calculate gradient\n",
        "def grad2(theta,X,y):\n",
        "    y_hat = sigmoid(np.dot(X, theta))\n",
        "    dJ = 1/m*np.sum((y_hat-y)*X,axis=0).reshape(-1,1)\n",
        "    return dJ\n",
        "def cost2(theta,X, y):         \n",
        "    z = np.dot(X, theta)\n",
        "    predict_1 = y * np.log(sigmoid(z))\n",
        "    predict_0 = (1 - y) * np.log(1 - sigmoid(z))\n",
        "    return -sum(predict_1 + predict_0) / len(X)\n",
        "\n",
        "def LR(X,y,learning_rate = 0.001,epochs=1000,TOL=1e-7,use_armijo = False):\n",
        "    theta = np.array([[0]]*X.shape[1])\n",
        "    print(theta)\n",
        "    theta_history = [theta]\n",
        "    J_history = [cost2(theta,X,y)]\n",
        "    \n",
        "    thetanew = theta\n",
        "    print(f'epoch \\t Cost2(J,X,y) \\t')\n",
        "    for epoch in range(epochs):\n",
        "        if epoch%100 == 0:\n",
        "            print(f'{epoch:5d}\\t{J_history[-1]}\\t')\n",
        "        dJ = grad2(theta,X,y)\n",
        "        J = cost2(theta,X,y)\n",
        "        if use_armijo:learning_rate= armijo(X,y,theta,grad2,cost2)\n",
        "        thetanew = theta - learning_rate*dJ\n",
        "        theta_history.append(thetanew)\n",
        "        J_history.append(J)\n",
        "        \n",
        "        if np.sum((thetanew - theta)**2) < TOL:\n",
        "            print('Convergence achieved.')\n",
        "            break\n",
        "        theta = thetanew\n",
        "\n",
        "    return thetanew,theta_history,J_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1MDDlnU5lpu"
      },
      "source": [
        "fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3HdHzLWNdfF"
      },
      "source": [
        "theta,theta_history,J_history = LR(X,ynorm,use_armijo=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4vNSUgg5sjU"
      },
      "source": [
        "Plot the diserid in blue, the predicted in red"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O_r5I6C5qMI"
      },
      "source": [
        "plt.scatter(x, ynorm)\n",
        "Y_pred = sigmoid( X.dot(theta))\n",
        "plt.scatter(x,Y_pred, color=\"red\") \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACCvpQDi57Sj"
      },
      "source": [
        "The losse **history**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwOe9zHhX3sS"
      },
      "source": [
        "plt.plot(J_history)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}